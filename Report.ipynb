{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Shixiang WANG\n",
    "\n",
    "**EID:** sxwang6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5487: Programming Assignment 2 Clustering - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Clustering synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "Implement the three clustering algorithms. You will reuse these algorithms in the next problem, so try to make them as general as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File Structure:\n",
    "    - preprocessing.py:\n",
    "        - data_loader : load the input data points\n",
    "    - cluster.py:\n",
    "        - Contains three cluster algorithm: 'kmeans' : Kmeans Algorithm, 'EM_GMM': EM algorithm for Gaussian mixture models, 'mean_shift' : Mean-shift algorithm.\n",
    "    - problem01.py:\n",
    "        - The running script to show the outcome of the above algorithms on the three synthetic datasets.\n",
    "    - problem02.py:\n",
    "        - The running script for the image segmentation task.\n",
    "    - images:\n",
    "        - Contains all original images(outcomes) I use to write the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "Run the algorithms on the three synthetic datasets. Qualitatively, how does each clustering algorithm perform on each dataset? Comment on the advantages and limitations of each algorithm, in terms of the conﬁguration of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/1b/original/Ground truth.png\" width = \"1120\" height = \"280\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/1b/kmeans/kmeans1.png\" width = \"1120\" height = \"280\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/1b/EM-GMM/gmm-300.png\" width = \"1120\" height = \"280\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/1b/Mean-shift/mean-shift-3.png\" width = \"1120\" height = \"280\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "- In this question, I tried three different clustering algorithms. The results can be seen above. Through observing the outcomes, I found their advantages and limitations:\n",
    "    - Kmeans:\n",
    "        - Advantages:\n",
    "            - It can model a cluster with a circular shape such as dataset 'A.'\n",
    "            - It runs faster than the other two algorithms. Its time complexity is O(tkn), where t is the iteration times, k is the number of centers and n is the number of points.\n",
    "            - Although, Kmeans mostly coverages to the local optimal solution, it is enough for us to use (this is just for the convex cluster shape). I ran the algorithm ten times. Only one time, it yielded poor clustering results, which could be seen above.\n",
    "        - Limitations:\n",
    "            - If the cluster shape is non-convex or non-spherical, Kmeans will give poor results such as the dataset 'B.' In other words, Kmeans cannot handle skewed clusters.\n",
    "            - It cannot model the non-compact clusters such as the dataset 'C,' either.\n",
    "            - If two clusters have the same center, K-means will fail.\n",
    "            - The initial centers could yield inferior results since many local minimums in the objective function. We have to try different initializations and choose the one that gives the lowest objective score.\n",
    "            - It is very sensitive to outliers. As k-means is based on the euclidean distance, few outliers could cause a large bias of the final centers. We have to preprocess these outliers and better do normalization.\n",
    "            - Kmeans is 'hard' assignment, this cause that it cannot do multi-cluster tasks. \n",
    "            - We need to select the value of K\n",
    "    - EM-GMM:\n",
    "        - Advantages:\n",
    "            - GMM could model a cluster with both an elliptical and a circular shape, like modeling the datasets 'B' and 'A,' respectively. The outcome is pretty good shown in the above figure. This is because we could use the covariance matrix of the Gaussian to control the ellipse shape. If we use the spherical covariance matrix, then the clusters' shape will be circular, which is the same as Kmeans.\n",
    "            - GMM is 'soft' assigment which means it can do multi-cluster tasks.\n",
    "        - Limitations:\n",
    "            - It cannot model the non-compact clusters such as the dataset 'C'.\n",
    "            - For high-dimensional data, the convariance matrix may be very large so the algorithm requires lots of data to learn effectively. (But in our case as the data dimension is only two, it does not matter.)\n",
    "            - We need to select the value of K.\n",
    "            - As GMM uses the EM algorithm, it may get bad results due to the local minimum. So we need to try many times to get the best one.\n",
    "            - GMM is also sensitve to the initialization. Bad initialization yields poor cluster results.\n",
    "    - Mean-shift:\n",
    "        - Advantages:\n",
    "            - It can model the concentrated compact datasets such like 'A' and 'B'.\n",
    "            - It can choose K via bandwidth parameter automatically.\n",
    "        - Limitations:\n",
    "            - It cannot model the non-compact clusters well such as the dataset 'C'.\n",
    "            - As the bandwidth implicitly controls the number of cluster centers, it is difficult for us to specify the number of center points. In our case, we know there are just four clusters, but it is tough for us to get the wanted cluster center number through tuning the bandwidth(In fact, I tried lots of times and all failed). The above figure shows the 'Mean-shift' algorithm gives us seven cluster centers on both datasets 'B' and 'C,' although the actual number is just four.\n",
    "            - It is very sensitive to the bandwith which can be seen in Q1(c).\n",
    "            - It runs very slow since it uses gradient ascent to get peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "How sensitive is mean-shift to the bandwidth parameter h?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/1b/Mean-shift/mean-shift-A.png\" width = \"1840\" height = \"270\"/>\n",
    "<img src=\"images/1b/Mean-shift/mean-shift-B.png\" width = \"1840\" height = \"270\"/>\n",
    "<img src=\"images/1b/Mean-shift/mean-shift-C.png\" width = \"1840\" height = \"270\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "- Here I tested five different bandwidth values(1, 2, 3, 5, 10) on all three datasets.\n",
    "- I found that the Mean-shift algorithm is very sensitive to the bandwidth as the number of clusters is controlled by the hypercube side. From the above figures we could get that:\n",
    "    - Smaller bandwidth will create more clusters no matter the shape of the clusters. As it fouces more on the local clusters.\n",
    "    - Larger bandwidth will create less clusters no matter the shape of the clusters. As it focuses more on the global clusters.\n",
    "    - We could get pretty good results on dataset c if we choose bandwidth values 5.\n",
    "- Conclusion: When we use mean-shift algorithm, we need to estimate the bandwidth well to get the best performance, and this is another disadvantage of this algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 A real world clustering problem – image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "\n",
    "Use the three clustering algorithms to segment a few of the provided images. Qualitatively, which algorithm gives better results? How do the results change with diﬀerent K and h? Which is less sensitive to changes in the parameters? Comment on any interesting properties or limitations observed about the clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/2a/kmeans/kmeans.png\" width = \"1280\" height = \"320\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/2a/gmm/gmm.png\" width = \"1280\" height = \"320\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/2a/mean-shift/mean-shift.png\" width = \"1280\" height = \"320\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "- I choose image '12003', '299086', '56028', '62096' to test the given three clustering algorithms. Figures above shows the evalution results.\n",
    "    - For Kmeans and EM-GMM, I tried three different number of clusters: 2, 3, and 5 respectively.\n",
    "    - For Mean-shift, I tried three different bandwidth: 0.3, 0.5, and 0.7 respectively.\n",
    "- In my case, EM-GMM and Mean-shift are better than Kmeans. As we can see that only the number of clusters is larger than five, Kmeans can ﬁnd the right homogeneous regions in the image if we use image '12003'. EM-GMM only needs two clusters. We could regard the number of clusters as the prior information. From this perspective, we could say that EM-GMM can learn more than Kmeans using the same data as we have to tell Kmeans more prior information. Mean-shift could give us more details than others by adjusting bandwidth values.\n",
    "- EM-GMM is less sensitive to its hyperparameter than Kmeans and Mean-shift. As we can see from the above figures, every time we change k or h, both Kmeans and Mean-shift's outcomes are different. However, for EM-GMM, changing k almost does not change its performance, as the Segments are good enough to show the shape of the original images.\n",
    "- Segments decided by Mean-shift give us many details and are very close to the original image compared to the other two. This is because that Mean-shift has more modes than others and more modes mean more different homogeneous regions. This also shows me that the Mean-shift algorithm does not consider the cluster's shape.\n",
    "- In fact, Kmeans runs much faster than the other two algorithms since it only has one step compared to EM-GMM, which has two steps in each iteration. Mean-shift runs the slowest. Although we change its bandwidth, it still needs to calculate the peak for every point, making it difficult for us to speed up the Mean-shift algorithm by adjusting the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) \n",
    "\n",
    "Modify your K-means and mean-shift implementations to allow diﬀerent feature scaling. Hint: changing the distance in (7) or kernel in (8) is equivalent to scaling each dimension in the feature vector x by an appropriate amount. Rerun the segmentation experiment. Do the segmentation results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/2b/kmeans/kmeans.png\" width = \"1280\" height = \"320\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "<img src=\"images/2b/mean-shift/mean-shift.png\" width = \"1280\" height = \"320\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "- I did zero-one normalization on the input data, then tested both Kmeans and Mean-shift, results can be seen as above.\n",
    "- Kmeans:\n",
    "    - We could find that Kmeans could get pretty good results starting with the number of centers being 3,  while before normalization, it needs five. \n",
    "    - It also shows us more details in the segments.\n",
    "- Mean-shift:\n",
    "    - If the input is normalized, Mean-shift could give us more detials. If not, the result is more blurry. This phenomenon is particularly evident in image '12003'.\n",
    "    - It runs faster than before.\n",
    "- Conclusion: It is necessary for algorithms based on Euclidean distance such as Kmeans and Mean-shift to do data normalization. If we do not, the dimension with a large range will affect the outcomes more. Doing normalization could scale down those features and make them less 'important'."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fddfbec118b8a6eec00f8005456499021538369a6963e34d464306f8d055c8f2"
  },
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
